from typing import Any, Union
import requests
import random
import jieba
import jieba.analyse
import time
from bs4 import BeautifulSoup
import re
import pickle
from selenium import webdriver
from requests import cookies
from nltk.corpus import sinica_treebank
import numpy as np


class data:
    def __init__(self, pid, cont, user_name, is_OP):
        self.postid = pid
        self.content = cont
        self.user = user_name
        self.op = is_OP

login_page = 'https://bbs.saraba1st.com/2b/member.php?mod=logging&action=login'

s1_url = 'https://bbs.saraba1st.com/2b/'

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/69.0.3497.92 Safari/537.36',
}



def save_cookies():
    driver = webdriver.Edge(executable_path='C:\\Users\\narya\\Documents\\MicrosoftWebDriver.exe')
    driver.get('https://bbs.saraba1st.com/2b/thread-1814684-1-1.html')
    driver.implicitly_wait(3)
    print("driver.get_cookies():")
    print(driver.get_cookies())
    saved_cookies = driver.get_cookies()
    return saved_cookies

def get_page(i):
    s = random.randint(4, 16)
    print('sleep for ' + str(s) + ' secs in get_page')
    time.sleep(s)
    preview_page_url = s1_url+ 'forum-75-'+ str(i) +'.html'
    #print(preview_page_url)
    waiye_soup = cook_soup(preview_page_url)
    preview_page_body = waiye_soup.body
    #print('length of preview_page_body: ')
    #print(preview_page_body)
    preview = preview_page_body.find('div', id='threadlist')
    #print('preview: ')
    #print(preview)
    threadnodes = []
    all_preview = preview.find_all('tbody')
    for preview in all_preview:
        tbody_id = preview['id']
        if tbody_id[:13] == 'normalthread_':
            threadnodes.append(preview)
    #return threadnodes
    thread_url = []
    for node in threadnodes:
        thread_suffix_elem = node.find('a')
        if thread_suffix_elem != -1:
            suffix = thread_suffix_elem['href']
            thread_url.append(s1_url + suffix)
    print(thread_url)
    return thread_url

def get_thread_url_from_preview(thread_preview):
    thread_suffix_elem = thread_preview.find('a')
    thread_url = ''
    if thread_suffix_elem != -1:
        suffix = thread_suffix_elem['href']
        thread_url = s1_url + suffix
    return thread_url


def cook_soup(url):
    #gingerbreadjar = save_cookies()
    #jar = requests.cookies.cookiejar_from_dict(gingerbreadjar)

    content = requests.get(url, headers = HEADERS)#, cookies = jar#)
    content.encoding = 'utf-8'
    print(content.status_code)
    s = random.randint(1,9)
    print('sleep for '+str(s)+' secs in cook_soup')
    time.sleep(s)
    soup = BeautifulSoup(content.text, "html.parser")
    return soup

def get_filtered_post(thread_url):
    cont_soup = cook_soup(thread_url)
    thread_body = cont_soup.body
    wp_div = thread_body.find('div', id='wp')
    print(wp_div)
    ct_div = wp_div.find('div', id='ct')
    print(ct_div)
    thread_post_list = ct_div.find('div', id='postlist')
    if thread_post_list is None:
        print('thread '+thread_url+' can not be reached')
        return []
    post_list = thread_post_list.find_all('div', id=re.compile("post_\d{8}"))
    data_list = []
    for post in post_list:
        # print(post)
        pid = post.find('table')['id']
        user = post.find('div', class_='authi').find('a').text
        content = filter_single_post(post.find('td', class_='t_f'))
        date = post.find.find('td', class_= 'plc').find('div', class_='authi').find('em')
        d = [pid, content, user, date]
        data_list.append(d)
    next_page = cont_soup.find('div', class_='pgbtn')
    if next_page != None:
        np_url_suffix = next_page.find('a')['href']
        np_url: Union[str, Any] = s1_url + np_url_suffix
        np_data_list = get_filtered_post(np_url)
        data_list += np_data_list
    #print(data_list)
    return data_list

def load_stopwords(filepath):
    stopwords = [line.strip() for line in open(filepath, 'r', encoding = 'utf-8').readlines()]
    stopwords[0] = ''
    #print(stopwords)
    return stopwords


def filter_single_post(post):
    q_tag = post.blockquote
    if q_tag != None:
        remove_quote = post.blockquote.extract()
    i_tag = post.i
    if i_tag != None:
        remove_pinfo = post.i.extract()
    device_info = post.a
    if device_info != None:
        remove_dinfo = post.a.extract()
    img_tag = post.img
    if img_tag != None:
        remove_img_info = post.img.extract()
    filtered_text = post.get_text(strip=True)
    filtered_data = ''.join(re.findall(r'[a-zA-Z0-9\u4e00-\u9fa5]', filtered_text))
    # print(filtered_data)
    return filtered_data

def get_all_post_filtered(post_list_unfiltered):
    filtered_posts_list = []
    for post in post_list_unfiltered:
        q_tag = post.blockquote
        if q_tag != None:
            remove_quote = post.blockquote.extract()
        i_tag = post.i
        if i_tag != None:
            remove_pinfo = post.i.extract()
        device_info = post.a
        if device_info !=None:
            remove_dinfo = post.a.extract()
        img_tag = post.img
        if img_tag != None:
            remove_img_info = post.img.extract()
        filtered_text = post.get_text(strip = True)
        filtered_data = ''.join(re.findall(r'[a-zA-Z0-9\u4e00-\u9fa5]', filtered_text))
        #print(filtered_data)
        filtered_posts_list.append(filtered_data)
    return filtered_posts_list

def get_thread_OG_data(thread_url):
    thread_soup = cook_soup(thread_url)
    thread_body = thread_soup.body
    hd = thread_body.find('div')
    post_list = thread_body.find('div', id='postlist')

def cut_contents(filtered_contents):
    words_list = []
    for c in filtered_contents:
        words_list += jieba.lcut(c)
    return words_list

def get_filtered_threads(threads_preview_list):
    testloop = 0
    thread_content_filtered = []
    for thread_preview in threads_preview_list:
        #print(testloop)
        testloop += 1
        thread_url = get_thread_url_from_preview(thread_preview)
        #print(thread_url)
        posts = get_filtered_post(thread_url)
        #content_filtered = get_all_post_filtered(posts)
        #fin = cut_contents(content_filtered)
        #thread_content_filtered += fin
        # print('new_list:')
        # print(new_list)
    return posts


def main():
    #filtered_all_posts = []
    #stop_words = load_stopwords('stopwords.txt')
    for i in range(2,17):
        filtered_all_posts = []
        threads_url_list = get_page(i)
        print("print url list")
        print(threads_url_list)
        for url in threads_url_list:
            print('processing '+url)
            filtered_all_posts += get_filtered_post(url)
    #clear stop words
            #print(filtered_all_posts)
            save_data(filtered_all_posts, i)


def test(url):
    i = 2
    filtered_all_contents = []
    posts = get_filtered_post(url)
    #content_filtered = get_all_post_filtered(posts)
    #filtered_all_contents = cut_contents(content_filtered)
    #stop_words = load_stopwords('stopwords.txt')
    #for cont in posts:
    #    cont[1] = jieba.lcut(cont[1])
    #    if cont[1] not in stop_words:
    #        if cont[1] != '\t':
    #            freq_dist_list.append(word)
    #freq_dist_posts = nltk.FreqDist(freq_dist_list)
    save_data(posts, 9)
    testload = load_data(9)
    for data in testload:
        print(data)


def save_data(obj, i):
    with open('page'+str(i)+'.plk', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)
        print('data in page '+str(i)+' is saved')

def load_data(i):
    with open('page'+str(i)+'.plk', 'rb') as f:
        return pickle.load(f)

#test('https://bbs.saraba1st.com/2b/thread-1814684-1-1.html')

def test_old_posts():
    l = get_page(9998)
    test_post = l[3]
    posts = get_filtered_post(test_post)
    print(posts)


test_old_posts()